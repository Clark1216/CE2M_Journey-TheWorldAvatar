- The test cases in this folder are for checking the code plumbing for all of the supported models (rf, svr, mlp).
- The tests also serve as a guide on how these models can be set up.
- The tests run on a fake dataset generated at runtime.
- Note that the purpose of these tests is NOT TO check the results as these are hardware specific.
- The tests also do not aim to produce a high quality models (the results will be very bad). This is so that the tests run quickly.
  However, before the tests were added it was checked that each model can be trained to a satisfactory level provided sufficient
  number of hpo trials was set (~150) and in case of mlp sufficiently large number of epochs (~150-200) was used.
- The tests DO CHECK if the correct files are generated and their sizes match the reference files.
- If you wish to work with the py4ml code and develop it further I would advise to enable strict results checking that is commented
  out by default in the test_models.py. This should be done on the machine you wish to do the development. Upon enabling it, the
  ref results should be first generated by copying the content of the test_data into the ref_data. Make sure you clean the ref_data
  prior copying. This is important for copying best trial retrain results as these generate best trial folder with the trial number
  that might be different on your machine. If the ref data are not cleaned it could contain two such folders with the ref results
  leading to problems.