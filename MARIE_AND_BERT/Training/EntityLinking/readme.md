## Overview
Two models are trained for Entity Linking:
1. A SMILES-NER model for SMILES string recognition.
2. The Entity Extraction module which performs joint Entity Recognition and Entity Linking. We use the method proposed in the BLINK project: https://github.com/facebookresearch/BLINK.

## Setup
1. Clone the BLINK project from https://github.com/facebookresearch/BLINK. 
2. Copy all the files in `Training/EntityLinking` to the top level directory of the BLINK project.
3. Install Python 3.8 and create the virtual environment as described in the [MARIE AND BERT README](../../readme.md#running) 

## Data Preparation
Please refer to [Dataset creation for Entity Linking](../../KGToolbox/EntityLinking/readme.md) to create the dataset required for training the Entity Linking models.

##Folder Structure
The expected folder structure after the set-up:

<pre>
├── blink-project-files...
├── trainbi.py    #Customized entry point to run BLINK training
├── evalbi.py     #Customized entry point to run BLINK evaluation
└── translator    # NER training code
    ├── util.py
    ├── train.py  #Entry point for NER train
    └── test.py   #Entry point for NER eval
└── my_scripts
    ├── train_entity_encoder.sh
    ├── train_question_encoder.sh
    └── train_ner.sh
</pre>

## Train SMILES NER
Run the following command to train the SMILES NER model:
```
bash train_ner.sh [valid_file_name] [train_file_name] 
```
* `--valid_file_name` Path to the validation `.jsonl` file.
* `--train_file_name` Path to the training `.jsonl` file.

Arguments can be supplied when calling bash to change the I/O paths. After running the script, the model is saved to the specified path (default is `./models/ner/SMILES_NER.bin`)  


## Train Entity Extraction
Entity Extraction training needs two steps.
First step:
```
bash train_entity_encoder.sh [data_path] [output_path]
```
To customize the I/O directorys:
* `--data_path` Path to first step data files. Suggested is `data/step1`.
* `--output_path` Path to output (step1) folder. Suggested is `models/step1`.


Second step:
```
bash train_question_encoder.sh [path_to_first_step_out] [data_path] [output_path]
```
* `--path_to_first_step_out` Path to output folder of step1. Suggested is `data/step1`.
* `--data_path` Path to second step data files. Suggested is `data/step2`.
* `--output_path` Path to output (step2) folder. Suggested is `models/step2`.




For example, the input folders are `./data/step1` for the first step and `./data/step2` for the second step; default output folders are `./models/step1` and `./models/step2` respectively.
Below would be an example of the default i/o structure. Files marked with # are input files; files marked with * are generated by the training process and are the final binary files required for Marie system running, which should be put under DATA/EntityLinking in the main project. Refer to MARIE_AND_BERT/readme.md to download an example of EntityLinking binary files. 
<pre>
├── project-files...
├── data
    ├── step1
         ├── dict.jsonl#
	 ├── train.jsonl#
	 ├── valid.jsonl#
    	 └── test.jsonl#
    ├── step2
         ├── dict.jsonl#
	 ├── train.jsonl#
	 ├── valid.jsonl#
    	 └── test.jsonl#
    └── ner
	 ├── train.jsonl#
	 ├── dev.jsonl#
    	 └── test.jsonl#
└── models
    ├── step1
         ├── ...
	 ├── id2text.json*
	 ├── id2title.json*
	 ├── id2wikidata.json*
    	 └── embeddingbase.pt*
    ├── step2
         ├── ...
	 ├── pytorch_model.bin*
    	 └── training_params.txt*
    └── ner
         └── SMILES_NER.bin*
</pre>

The location of these files in EntityLinking.zip are:
<pre>
├── EntityLinking
    ├── ...
    ├── models
         ├── id2text.json
         ├── id2title.json
         └── id2wikidata.json
    ├── pytorch_model.bin
    ├── embeddingbase.pt
    ├── training_params.txt
    └── SMILES_NER.bin*

</pre>